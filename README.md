# StockTwits Search Engine Team-RSJB CS410

# Project Video
https://drive.google.com/file/d/1-M08jBvJin0lV7XjcFDc5k3yOELH6CJ7/view

# Project Overview
	Team RSJB developed a search engine for StockTwits so that users can now search for terms, phrases or any other unique phrases to find content on that website. The search engine is built on a BM25 search algorithm, a Flask front end and the dataset was generated utilizing the requests and BS4 python libraries. The Flask web app relies on user input that connects with the BM25 search algo hosted as a Google Cloud Platform cloud function and returns the top 15 relevant results. To continue gathering new data a scraper is also deployed as a GCP cloud function that runs hourly to continually add to our dataset.
	The code structure is as follows. The Flask web app is located in src/rsjb_WebApp. Depending on what environment this will be run on a requirements.txt is also attached and can be installed via pip install -r requirements.txt or python/python3 -m pip install requirements.txt. This is dependent on the python version installed. It also may be helpful installing into a virtual environment and instructions can be found here if wanted: https://docs.python.org/3/library/venv.html. After installing all dependencies you can run the flask app by running: python/python3 TwitApp.py - again dependent on the python version. Once run you can access the application by going to a browser and to localhost:5000. Once there, search a query of your choice and the top 15 results will be displayed once the function returns relevant data. In src/search_algorithm has a jupyter notebook with instructions on how the search algo was designed, tested, and shown how it is working. In src/scripts includes jupyter notebooks with research and implementation of how the scraper was designed, tested and implemented with instructions. In src/cloud it includes f1.py and f2.py which are the functions that are currently running in the cloud and are adaptations of the search algo and scraper in the previous two folders. Also inside that folder are the scraper jupyter notebooks that were running locally to generate the 200,000 good data points for the search algo to generate results based on. Going through that jupyter notebook you will go in order to first install the required pip libraries, then import, run google credentials, then start the function. If any errors arise while starting the application or various parts of the codebase it will most probably be due to not having sufficient pip modules installed and the error should reflect that. So if it that error arises then run pip install <missing_import_library> and this should remove that error for that missing library.
	William Skedd was responsible for connecting the various pieces of the code base together, along with making adjustments to existing code to ensure code will correctly run, debugging the code where needed, and running scrapers on a local machine to generate a dataset large enough for the search algorithm. Time was first spent researching on how to connect all the different pieces of our code requirements together. The decision after research was to use Google Cloud Platform for cloud functionalities and Firestore to store index related variables for the search algorithm and Firestore storage to store our CSV’s with scraped data. After the search algorithm was finished by Sam, William then designed the CSV that would hold all of the scraped twits data along with the headers for the CSV. William then had to go through debugging and getting familiar with GCP cloud functions to deploy the search algorithm in GCP cloud. After getting the function deployed adjustments were needed to get the query from the data params of the function and that be used for the input into the search algorithm. Then to convert the returned values into an array of jsons to be returned back to the front end. This is in cloud/f1.py. Then after Jeremy had concluded his work on the scraping William made adjustments to the scraping functionalities to add in more data to be included in the CSV for the search algorithm as well as error handling and debugging while deploying the function to GCP cloud - cloud/f2.py. This also included getting the function to run on an hourly basis to add new data to the dataset CSV. As the functions in the cloud could not be run for long times and could run up cost if run more frequently William made local scrapers and spent time running them and keeping them running for around 500,000 id’s and gathering around 200,000 good data points for the search algo CSV. William made sure the scraper’s were always running and handling errors as they came - cloud/scraper(2,3,4,5).ipynb. William made - cloud/combine_csvs.ipynb - to combine the different CSVs that were generated from each of the local scrapers that were generated and stored in the Firestore Storage, and this function would then combine and save as the CSV that is being used in the search algo cloud function. Lastly, William researched how to call the search algo through a post request using google auth libraries in python to access the function and send corresponding query data to the function and how to handle response for the front end of the application. William wrote his code using python and jupyter notebook format and utilized the google auth python libraries to make secured requests to the cloud functions when querying for the search algo. Also included were the requests and bs4 libraries when working with scraping functionalities. William also used Google Cloud Platform, Firestore and Firestore Storage to host all of the different functionality of the backend system for the project.
	Jeremy Bao was responsible for writing the code used for scraping posts from StockTwits. First, he spent some time reading tutorials on how to obtain information from websites, as he did not have any experience with this (aside from a single workshop on web scraping attended 5 years ago). While teammates had previously stated that Selenium and BeautifulSoup should be used, he determined that it would be sufficient to use the “requests” library to get the contents of web pages and BeautifulSoup to extract information from them. He discussed with his group members how the posts on StockTwits could be sequentially crawled, and determined that each post could be accessed by going to “https://stocktwits.com/message/<some integer>”, where each post is given a unique id number, and these id numbers are assigned sequentially. So, the first post ever made could be found at “https://stocktwits.com/message/1”, the second post could be found at “https://stocktwits.com/message/2”, and so on, though many of the early posts had been deleted (note that although when you navigate through StockTwits manually, the username of each post’s author will be found between the “.com” and the “message”, so you might get something like “https://stocktwits.com/Prospectus/message/4”, the usernames were not initially needed to reach the web pages containing each post). He also found (by creating a StockTwits account and creating a post) that it was impossible to edit StockTwits posts, so that we would only have to store each post’s contents once. His initial explorations were done in “/src/scripts/Try_Beautiful_Soup.ipynb”, where he played around with web scraping, figured out how to extract the text from StockTwits posts (this involved some usage of the “Inspect Element” tool provided by various browsers), and investigated how dates could be extracted from those posts (they could be successfully extracted from older posts, but not ones made in the past few days). Jeremy did all of his coding online, in Google Colab.
	William then created “/src/scripts/twitsscraper.ipynb”, in which he extracted authors from StockTwits posts and created code to store information about StockTwits posts and flags for where crawling should start. Jeremy then came to have another look at his code, and realized that StockTwits changed their addressing scheme, thus breaking our previous code. Now, entering “https://stocktwits.com/message/<N>”, where N is some integer, would not take you to the N-th post submitted to StockTwits. You could still access each post if you knew the username of its author (by going to “https://stocktwits.com/<username>/message/<N>”), but we did not know which user had authored each post. Jeremy investigated various alternative approaches. Getting a token to use StockTwit’s API would be difficult, and he could not find any information on their site about it. He also read that even with a token, only 500 requests could be made per hour using that API, which would be insufficient. He also looked into the “pytwits” library. However, that library was poorly documented, he could not get it to work, and it seemed to require an API token to run. Jeremy spent some time looking around the internet and throughout StockTwit’s website for a solution.
Eventually, he randomly decided to paste “https://stocktwits.com/user/message/4” into the address bar and successfully accessed the fourth post ever made to StockTwits. Apparently, you now had to put “user” (or any other text) between “.com” and “message” in order to access StockTwits posts without entering a username. He worked on scraping information from these posts in “/src/scripts/twitsscraper_1_1.ipynb”, where he created code to iterate through many different posts until 1,000 non-deleted posts had been retrieved and then store the post ids, text content, and authors of those posts. One complication was encountered at this point, as the method William had developed to scrape post authors no longer worked. While post authors could be found in each post’s HTML representation in multiple places when accessing posts via a web browser, they could only be found in JSON stored within a “script” element with an “id” of “__NEXT_DATA__”  when accessing these posts using requests and BeautifulSoup. The “json” library had to be used to obtain information from these JSON objects. Jeremy recorded how long it took to crawl 1,000 non-deleted posts and found that it took a bit over 5 minutes, which led to our group deciding not to crawl every single post out of the ~500 million currently on StockTwits (as it would take too long). He also suggested that we store only post id numbers, authors, and text in order to save storage space and reduce complexity, as the other information could be computed from those values easily, but the others decided to store more information.
	Jeremy’s code was written in Python and contained within Jupyter notebooks. It used the “requests” library to get the web pages containing StockTwits posts, the BeautifulSoup library to scrape information from them, the “json” library to deal with a JSON object stored within the HTML representations of each web page, the “csv” library to store information to CSV files, and the “time” library to record how long things took to run. “firebase_admin” was imported, though it was mostly just used for William’s exploratory code. “pandas” was imported because Jeremy used it while following a web scraping tutorial, and was used in the version of his code deployed to Google Cloud Platform. Jeremy’s code can be found in “src/scripts” within our repository. It is not run during the normal operation of our web application, but the code used to scrape information from StockTwits and store it on Google Firestore was based off of it.
Sewoong (Sam)Lee was responsible for developing the search algorithm of the search engine. In order to use the Okapi BM25 algorithm, he researched and learned libraries for ranking algorithms. He finally selected rank-bm25 for this project, comparing API structure. By wrapping libraries and adding features, the class of search algorithm provides features such as updating ranker, calculating scores, printing and plotting top search results for given corpus and query. Pandas, NumPy, and matplotlib are also used in developing the algorithm. Then he connected his algorithm with the formatted CSV file, so that teammates can easily use the algorithm. After making the algorithm compatible with the CSV files, he also implemented the interfacing code with JSON objects. To be used in web applications, the algorithm returns its search result in the array of JSON objects and JSON file of which filename extension is ".json". To sum up, the input of the algorithm is given in CSV format and returns search results in JSON format. The example of converting JSON file into pandas dataframe is also implemented. Lastly he made documentation and summarized results in the Jupyter Notebook format for readability. 
	Ritik was responsible for creating the Web application using the Flask web framework along with HTML and CSS. Initially, he took time to learn and understand flask commands and how they are implemented.
The flask application ‘TwitApp.py’ created by Ritik consists of two html pages called ‘index.html’ and ‘data.html’ along with CSS style files. 'index.html’ is the page where the user enters their query and ‘data.html’ is the page where the top 15 search results of the query entered by the user are displayed.
The basic structure of the Flask application is as follows:
1)  The flask program is executed and it creates a website with a specific URL (currently localhost).
2) When a User enters the specific URL, they are directed to the main flask application page (which is an html page called index.html).
3) Once in the webpage, the user can enter their respective query that they want to search, in the provided text field.
4) After clicking the Search button, the flask application takes this query and sends it to the search algorithm in the google cloud platform (GCP).
5) The algorithm then sends the top 15 results of that query search back to the flask application.
6) We then perform some processing on the results to convert it into a presentable format and display it to the user (results are displayed using an html page called data.html).
7) The user can now view these results. If the user wishes to do a search using another query, then they can make use of the button labeled “Go Back” to return to the main application window where they can enter another query. 

